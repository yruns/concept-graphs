

# QueryScene: 查询驱动的多模态场景表示

## 一、方法概述

### 1.1 核心理念

**QueryScene** 是一种为VLM推理优化的新型场景表示，核心思想是：

> **预构建丰富的多模态场景索引，在查询时高效检索最相关的视觉-语言证据，供VLM进行精准推理。**

### 1.2 与现有方法的本质区别

```
┌─────────────────────────────────────────────────────────────────────┐
│                       方法范式对比                                   │
├─────────────────────────────────────────────────────────────────────┤
│                                                                     │
│  传统场景图 (HOV-SG, ConceptGraphs)                                 │
│  ┌─────────────────────────────────────────────────────────────┐   │
│  │  设计目标: 为图遍历/结构化查询优化                           │   │
│  │  输出形式: 节点-边图结构                                     │   │
│  │  查询方式: 图路径查找、子图匹配                              │   │
│  │  局限性:   预定义类别、固定关系、难接VLM                     │   │
│  └─────────────────────────────────────────────────────────────┘   │
│                                                                     │
│  纯特征匹配 (OpenScene, LERF)                                       │
│  ┌─────────────────────────────────────────────────────────────┐   │
│  │  设计目标: 开放词汇定位                                      │   │
│  │  输出形式: 相似度热力图                                      │   │
│  │  查询方式: 向量相似度搜索                                    │   │
│  │  局限性:   无空间推理、无法接VLM、词袋问题                   │   │
│  └─────────────────────────────────────────────────────────────┘   │
│                                                                     │
│  QueryScene (本方法)                                                │
│  ┌─────────────────────────────────────────────────────────────┐   │
│  │  设计目标: 为VLM推理优化                                     │   │
│  │  输出形式: VLM证据包 (图像 + 文本描述 + 空间信息)            │   │
│  │  查询方式: 层次化检索 + 视角选择 + VLM推理                   │   │
│  │  优势:     开放词汇、空间推理、原生VLM接口、可解释           │   │
│  └─────────────────────────────────────────────────────────────┘   │
│                                                                     │
└─────────────────────────────────────────────────────────────────────┘
```

---

## 二、场景表示的完整数据结构

### 2.1 四层表示架构

```
┌─────────────────────────────────────────────────────────────────────┐
│                    QueryScene 四层表示架构                           │
├─────────────────────────────────────────────────────────────────────┤
│                                                                     │
│  ┌──────────────┐  ┌──────────────┐  ┌──────────────┐  ┌─────────┐ │
│  │   几何层     │  │   语义层     │  │   视觉层     │  │  索引层 │ │
│  │  Geometry    │  │  Semantics   │  │   Visual     │  │  Index  │ │
│  ├──────────────┤  ├──────────────┤  ├──────────────┤  ├─────────┤ │
│  │              │  │              │  │              │  │         │ │
│  │ • 完整点云   │  │ • 物体标签   │  │ • RGB-D序列  │  │ • CLIP  │ │
│  │ • 物体点云   │  │   (GDino)    │  │ • 俯视图BEV  │  │   索引  │ │
│  │ • 墙体检测   │  │ • 功能描述   │  │ • 物体裁剪   │  │ • 视角  │ │
│  │ • 区域划分   │  │ • 空间描述   │  │ • 区域视图   │  │   索引  │ │
│  │ • 相机位姿   │  │ • 上下文描述 │  │ • 深度图     │  │ • 空间  │ │
│  │ • 3D边界框   │  │ • 区域描述   │  │              │  │   索引  │ │
│  │              │  │              │  │              │  │         │ │
│  └──────────────┘  └──────────────┘  └──────────────┘  └─────────┘ │
│         │                 │                 │               │       │
│         └─────────────────┴─────────────────┴───────────────┘       │
│                                    │                                │
│                          统一的查询接口                              │
│                                                                     │
└─────────────────────────────────────────────────────────────────────┘
```

### 2.2 完整数据结构定义

```python
@dataclass
class QuerySceneRepresentation:
    """QueryScene的完整场景表示"""
    
    # ==================== 几何层 ====================
    point_cloud: PointCloud              # 完整3D点云 (N x 6: xyz + rgb)
    camera_poses: List[CameraPose]       # 相机位姿序列
    wall_segments: List[WallSegment]     # 墙体检测结果
    floor_plan: np.ndarray               # 2D地面多边形
    
    # ==================== 物体层 ====================
    objects: List[ObjectNode]            # 物体列表
    
    # ==================== 区域层 ====================
    regions: List[RegionNode]            # 区域列表 (基于墙体/功能划分)
    
    # ==================== 视觉层 ====================
    rgb_images: List[np.ndarray]         # RGB图像序列
    depth_maps: List[np.ndarray]         # 深度图序列
    bev_image: np.ndarray                # 俯视图 (带语义标注)
    
    # ==================== 索引层 ====================
    clip_index: HierarchicalCLIPIndex    # 层次化CLIP索引
    visibility_index: VisibilityIndex    # 视角-物体双向索引
    spatial_index: SpatialIndex          # 空间索引 (KD-Tree)


@dataclass
class ObjectNode:
    """物体节点 - 整合SAM分割和GroundingDINO检测结果"""
    
    # === 标识 ===
    obj_id: int
    
    # === 几何信息 ===
    point_cloud: PointCloud              # 物体点云 (来自SAM + 深度)
    bbox_3d: BoundingBox3D               # 3D边界框
    centroid: np.ndarray                 # 质心 [x, y, z]
    
    # === 语义信息 (来自GroundingDINO) ===
    category: str                        # 物体类别 "lamp", "sofa"
    detection_confidence: float          # 检测置信度
    
    # === 多层次描述 (预构建) ===
    descriptions: ObjectDescriptions
    
    # === 特征 ===
    clip_feature: np.ndarray             # 聚合CLIP特征 (512,)
    text_embedding: np.ndarray           # 描述文本嵌入 (768,)
    
    # === 视觉信息 ===
    best_view_ids: List[int]             # 最佳观察视角ID列表
    crop_images: Dict[int, np.ndarray]   # {view_id: 裁剪图像}
    
    # === 空间关系 ===
    region_id: int                       # 所属区域ID
    nearby_object_ids: List[int]         # 邻近物体ID列表


@dataclass
class ObjectDescriptions:
    """物体的多层次自然语言描述"""
    
    appearance: str      # 外观: "白色金属底座，圆形米色灯罩"
    function: str        # 功能: "可调节亮度的阅读台灯"
    spatial: str         # 空间: "位于灰色沙发左侧约50cm处，高度约60cm"
    context: str         # 上下文: "与旁边的书架、沙发共同构成阅读角"
    summary: str         # 摘要: "沙发旁的白色阅读台灯"


@dataclass  
class RegionNode:
    """区域节点 - 基于墙体划分或功能聚类"""
    
    # === 标识 ===
    region_id: int
    
    # === 几何信息 ===
    boundary_2d: Polygon                 # 2D边界多边形
    point_cloud: PointCloud              # 区域内点云
    area: float                          # 面积 (m²)
    
    # === 包含内容 ===
    object_ids: List[int]                # 区域内物体ID列表
    
    # === 语义信息 ===
    region_type: str                     # "living_area", "reading_corner", "dining_area"
    description: str                     # "包含沙发、茶几、电视的客厅休息区"
    
    # === 特征 ===
    clip_feature: np.ndarray             # 区域聚合CLIP特征
    
    # === 视觉信息 ===
    bev_crop: np.ndarray                 # 俯视图中该区域的裁剪
    representative_view_ids: List[int]   # 代表性视角ID列表
```

---

## 三、层次化索引设计

### 3.1 三级CLIP索引

```
┌─────────────────────────────────────────────────────────────────────┐
│                    层次化CLIP索引架构                                │
├─────────────────────────────────────────────────────────────────────┤
│                                                                     │
│  查询: "沙发旁边的台灯"                                              │
│         │                                                           │
│         ▼                                                           │
│  ┌─────────────────────────────────────────────────────────────┐   │
│  │  Level 1: 区域索引 (FAISS IVF-Flat)                          │   │
│  │  ┌─────────────────────────────────────────────────────┐     │   │
│  │  │  输入: query_embedding                               │     │   │
│  │  │  搜索: 20个区域的聚合特征                            │     │   │
│  │  │  输出: Top-3 候选区域 [客厅, 卧室, 书房]             │     │   │
│  │  │  耗时: ~1ms                                          │     │   │
│  │  └─────────────────────────────────────────────────────┘     │   │
│  └─────────────────────────────────────────────────────────────┘   │
│         │                                                           │
│         ▼ (仅在候选区域内搜索)                                      │
│  ┌─────────────────────────────────────────────────────────────┐   │
│  │  Level 2: 物体索引 (FAISS IVF-PQ)                            │   │
│  │  ┌─────────────────────────────────────────────────────┐     │   │
│  │  │  输入: query_embedding + region_filter               │     │   │
│  │  │  搜索: 候选区域内的~50个物体                         │     │   │
│  │  │  输出: Top-10 候选物体 [台灯A, 台灯B, 沙发, ...]     │     │   │
│  │  │  耗时: ~2ms                                          │     │   │
│  │  └─────────────────────────────────────────────────────┘     │   │
│  └─────────────────────────────────────────────────────────────┘   │
│         │                                                           │
│         ▼ (可选: 需要部件级定位时)                                  │
│  ┌─────────────────────────────────────────────────────────────┐   │
│  │  Level 3: 点级索引 (局部精确搜索)                            │   │
│  │  ┌─────────────────────────────────────────────────────┐     │   │
│  │  │  输入: query_embedding + object_filter               │     │   │
│  │  │  搜索: 候选物体内的~5000个点                         │     │   │
│  │  │  输出: 高相似度点云区域                              │     │   │
│  │  │  耗时: ~5ms                                          │     │   │
│  │  └─────────────────────────────────────────────────────┘     │   │
│  └─────────────────────────────────────────────────────────────┘   │
│                                                                     │
│  总耗时: 3-8ms (vs 全点云搜索 ~50ms)                                │
│                                                                     │
└─────────────────────────────────────────────────────────────────────┘
```

### 3.2 视角-物体双向索引

#### 设计动机：几何可见性 vs 语义代表性

传统方法（如HOV-SG使用kmeans聚类）仅考虑几何因素选择视角。但一个"几何上看得清楚"的视角不一定是"语义上最具代表性"的视角：

```
┌─────────────────────────────────────────────────────────────────────┐
│                    视角选择的两个维度                                │
├─────────────────────────────────────────────────────────────────────┤
│                                                                     │
│  几何因素 (传统):                    语义因素 (新增):               │
│  ┌─────────────────────────┐        ┌─────────────────────────┐    │
│  │ • 可见比例              │        │ • CLIP语义相似度        │    │
│  │ • 观察角度              │        │   crop_image ↔          │    │
│  │ • 分辨率                │        │   "a photo of a chair"  │    │
│  │ • 遮挡程度              │        │                         │    │
│  └─────────────────────────┘        └─────────────────────────┘    │
│              ↓                                ↓                     │
│  "这个视角看得清楚吗？"           "这个视角看起来像椅子吗？"        │
│                                                                     │
│  示例: 椅子的不同视角                                               │
│  ┌─────────┐  ┌─────────┐  ┌─────────┐                             │
│  │  正面   │  │  背面   │  │  底部   │                             │
│  │  ████   │  │  ████   │  │  ════   │                             │
│  │ ██████  │  │ ██████  │  │ ══════  │                             │
│  └─────────┘  └─────────┘  └─────────┘                             │
│   几何: 0.9     几何: 0.85    几何: 0.7                             │
│   语义: 0.95    语义: 0.6     语义: 0.3   ← 差异显著！              │
│                                                                     │
│  结论: 正面视角在语义上更具代表性，VLM更容易识别                    │
│                                                                     │
└─────────────────────────────────────────────────────────────────────┘
```

#### 实现代码

```python
class VisibilityIndex:
    """视角与3D内容的双向索引 - 结合几何可见性和语义相似度"""
    
    def __init__(self):
        # === 正向索引: 3D内容 → 视角 ===
        # object_id → [(view_id, ViewScore), ...]
        self.object_to_views: Dict[int, List[Tuple[int, ViewScore]]] = {}
        # region_id → [(view_id, coverage_ratio), ...]
        self.region_to_views: Dict[int, List[Tuple[int, float]]] = {}
        
        # === 反向索引: 视角 → 3D内容 ===
        # view_id → [(object_id, ObjectVisibility), ...]
        self.view_to_objects: Dict[int, List[Tuple[int, ObjectVisibility]]] = {}
        
    def build(self, objects: List[ObjectNode], 
              regions: List[RegionNode],
              camera_poses: List[CameraPose], 
              depth_maps: List[np.ndarray],
              rgb_images: List[np.ndarray],
              K: np.ndarray,
              clip_model):
        """离线构建双向索引 - 结合几何可见性和语义相似度"""
        
        # 预计算所有物体类别的文本特征
        category_text_features = {}
        for obj in objects:
            if obj.category not in category_text_features:
                text_prompt = f"a photo of a {obj.category}"
                category_text_features[obj.category] = clip_model.encode_text(text_prompt)
        
        for view_id, (pose, depth, rgb) in enumerate(zip(camera_poses, depth_maps, rgb_images)):
            for obj in objects:
                visibility = self._compute_object_visibility(obj, pose, depth, K)
                
                if visibility.visible_ratio > 0.1:  # 至少10%可见
                    # 计算语义相似度: 裁剪图像 vs 类别文本
                    semantic_score = self._compute_semantic_score(
                        obj, pose, rgb, K, 
                        category_text_features[obj.category],
                        clip_model
                    )
                    
                    # 双向记录 (几何+语义)
                    view_score = ViewScore(
                        view_id=view_id,
                        visible_ratio=visibility.visible_ratio,
                        view_quality=visibility.view_quality,
                        resolution=visibility.resolution,
                        occlusion_ratio=visibility.occlusion_ratio,
                        semantic_score=semantic_score  # 新增语义得分
                    )
                    self.object_to_views[obj.obj_id].append((view_id, view_score))
                    self.view_to_objects[view_id].append((obj.obj_id, visibility))
        
        # 对每个物体的视角按综合质量排序
        for obj_id in self.object_to_views:
            self.object_to_views[obj_id].sort(
                key=lambda x: x[1].get_composite_score(), reverse=True
            )
    
    def _compute_semantic_score(self, obj: ObjectNode, pose: CameraPose, 
                                 rgb: np.ndarray, K: np.ndarray,
                                 category_text_feature: np.ndarray,
                                 clip_model) -> float:
        """计算物体裁剪图与类别文本的CLIP相似度"""
        
        # 1. 将物体3D边界框投影到2D
        bbox_2d = project_3d_bbox_to_2d(obj.bbox_3d, pose, K)
        if bbox_2d is None:
            return 0.0
        
        # 2. 裁剪物体区域 (带padding)
        x1, y1, x2, y2 = bbox_2d
        pad = int(0.1 * max(x2 - x1, y2 - y1))  # 10% padding
        x1, y1 = max(0, x1 - pad), max(0, y1 - pad)
        x2, y2 = min(rgb.shape[1], x2 + pad), min(rgb.shape[0], y2 + pad)
        
        crop = rgb[y1:y2, x1:x2]
        if crop.size == 0:
            return 0.0
        
        # 3. 计算裁剪图的CLIP特征
        crop_feature = clip_model.encode_image(crop)
        
        # 4. 计算与类别文本的余弦相似度
        similarity = cosine_similarity(crop_feature, category_text_feature)
        
        return float(similarity)
    
    def get_best_views_for_objects(self, 
                                    object_ids: List[int], 
                                    criteria: str = "joint_coverage",
                                    max_views: int = 3) -> List[int]:
        """获取能同时观察到多个物体的最佳视角"""
        
        if criteria == "joint_coverage":
            # 贪心选择: 最大化联合覆盖
            return self._greedy_joint_coverage(object_ids, max_views)
        elif criteria == "best_per_object":
            # 每个物体的最佳视角
            views = set()
            for obj_id in object_ids:
                if self.object_to_views[obj_id]:
                    views.add(self.object_to_views[obj_id][0][0])
            return list(views)[:max_views]
    
    def _greedy_joint_coverage(self, object_ids: List[int], k: int) -> List[int]:
        """贪心算法: 选择k个视角最大化所有目标物体的联合覆盖"""
        
        # 收集所有候选视角
        candidate_views = set()
        for obj_id in object_ids:
            for view_id, _ in self.object_to_views.get(obj_id, []):
                candidate_views.add(view_id)
        
        selected = []
        covered_quality = {obj_id: 0.0 for obj_id in object_ids}
        
        for _ in range(k):
            best_view, best_gain = None, 0
            
            for view_id in candidate_views - set(selected):
                # 计算选择该视角的边际增益
                gain = 0
                for obj_id in object_ids:
                    view_score = self._get_view_score(obj_id, view_id)
                    if view_score and view_score.get_composite_score() > covered_quality[obj_id]:
                        gain += view_score.get_composite_score() - covered_quality[obj_id]
                
                if gain > best_gain:
                    best_gain, best_view = gain, view_id
            
            if best_view is None:
                break
                
            selected.append(best_view)
            # 更新覆盖质量
            for obj_id in object_ids:
                view_score = self._get_view_score(obj_id, best_view)
                if view_score:
                    covered_quality[obj_id] = max(
                        covered_quality[obj_id], 
                        view_score.get_composite_score()
                    )
        
        return selected


@dataclass
class ViewScore:
    """视角质量评分 - 结合几何因素和语义因素"""
    view_id: int
    
    # === 几何因素 ===
    visible_ratio: float      # 物体可见点比例 [0, 1]
    view_quality: float       # 观察角度质量 (正视=1, 侧视=0)
    resolution: float         # 有效分辨率 (像素/米)
    occlusion_ratio: float    # 被遮挡比例 [0, 1]
    
    # === 语义因素 (新增) ===
    semantic_score: float     # 物体裁剪图与类别文本的CLIP相似度 [0, 1]
    
    def get_composite_score(self, weights=None) -> float:
        """计算综合得分 - 几何+语义"""
        if weights is None:
            weights = {
                "visible": 0.25, 
                "quality": 0.15, 
                "resolution": 0.2, 
                "occlusion": 0.15,
                "semantic": 0.25  # 语义相似度权重
            }
        
        # 归一化resolution到[0,1]
        norm_resolution = min(self.resolution / 100, 1.0)
        
        return (weights["visible"] * self.visible_ratio +
                weights["quality"] * self.view_quality +
                weights["resolution"] * norm_resolution +
                weights["occlusion"] * (1 - self.occlusion_ratio) +
                weights["semantic"] * self.semantic_score)
```

---

## 四、查询处理流程

### 4.1 完整处理流程

```
┌─────────────────────────────────────────────────────────────────────┐
│                    QueryScene 查询处理流程                           │
├─────────────────────────────────────────────────────────────────────┤
│                                                                     │
│  输入: "沙发旁边的台灯"                                              │
│         │                                                           │
│  ═══════╪═══════════════════════════════════════════════════════   │
│         ▼                                                           │
│  Step 1: 查询解析 (LLM)                                             │
│  ┌─────────────────────────────────────────────────────────────┐   │
│  │  输入: "沙发旁边的台灯"                                      │   │
│  │  输出: {                                                     │   │
│  │    "target": "台灯",                                         │   │
│  │    "anchor": "沙发",                                         │   │
│  │    "relation": "旁边",                                       │   │
│  │    "query_type": "spatial_relation",                         │   │
│  │    "use_bev": true   // LLM自动推断是否需要俯视图            │   │
│  │  }                                                           │   │
│  │                                                              │   │
│  │  use_bev 推断规则 (LLM内部判断):                             │   │
│  │  • 涉及水平空间关系 (旁边/之间/周围) → true                  │   │
│  │  • 涉及区域/功能区识别 → true                                │   │
│  │  • 计数任务 → true                                           │   │
│  │  • 简单物体定位 → false                                      │   │
│  │  • 垂直关系 (上面/下面) → false                              │   │
│  │  • 外观属性查询 → false                                      │   │
│  └─────────────────────────────────────────────────────────────┘   │
│         │                                                           │
│  ═══════╪═══════════════════════════════════════════════════════   │
│         ▼                                                           │
│  Step 2: 层次化检索                                                  │
│  ┌─────────────────────────────────────────────────────────────┐   │
│  │  2a. 编码查询:                                               │   │
│  │      target_emb = CLIP.encode("台灯")                        │   │
│  │      anchor_emb = CLIP.encode("沙发")                        │   │
│  │                                                              │   │
│  │  2b. 区域级检索:                                             │   │
│  │      candidate_regions = region_index.search(target_emb, k=3)│   │
│  │      → [客厅, 卧室]                                          │   │
│  │                                                              │   │
│  │  2c. 物体级检索 (在候选区域内):                              │   │
│  │      target_candidates = object_index.search(                │   │
│  │          target_emb, region_filter=candidate_regions)        │   │
│  │      → [台灯A, 台灯B, 落地灯]                                │   │
│  │                                                              │   │
│  │      anchor_candidates = object_index.search(                │   │
│  │          anchor_emb, region_filter=candidate_regions)        │   │
│  │      → [沙发A, 沙发B]                                        │   │
│  └─────────────────────────────────────────────────────────────┘   │
│         │                                                           │
│  ═══════╪═══════════════════════════════════════════════════════   │
│         ▼                                                           │
│  Step 3: 空间约束过滤 (几何计算)                                    │
│  ┌─────────────────────────────────────────────────────────────┐   │
│  │  对每个 (target, anchor) 组合计算空间关系:                   │   │
│  │                                                              │   │
│  │  for target in target_candidates:                            │   │
│  │      for anchor in anchor_candidates:                        │   │
│  │          distance = ||target.centroid - anchor.centroid||    │   │
│  │          if relation == "旁边" and distance < 1.5m:          │   │
│  │              valid_pairs.append((target, anchor, distance))  │   │
│  │                                                              │   │
│  │  结果: [(台灯A, 沙发A, 0.6m), (台灯B, 沙发A, 1.2m)]          │   │
│  └─────────────────────────────────────────────────────────────┘   │
│         │                                                           │
│  ═══════╪═══════════════════════════════════════════════════════   │
│         ▼                                                           │
│  Step 3.5: 俯视图空间筛选 [可选分支, 当 use_bev=true 时执行]        │
│  ┌─────────────────────────────────────────────────────────────┐   │
│  │  目的: 利用俯视图的全局视角进一步筛选候选                    │   │
│  │                                                              │   │
│  │  3.5a. 生成带标注的俯视图:                                   │   │
│  │        annotated_bev = annotate_bev(                         │   │
│  │            bev_image,                                        │   │
│  │            anchor=沙发A,           # 红色高亮                │   │
│  │            candidates=[台灯A, 台灯B],  # 蓝色标注            │   │
│  │            distance_rings=[1m, 2m]  # 灰色距离参考圈         │   │
│  │        )                                                     │   │
│  │                                                              │   │
│  │  3.5b. VLM俯视图分析:                                        │   │
│  │        bev_prompt = "俯视图中红色是沙发，蓝色是候选台灯。    │   │
│  │                      根据位置关系，哪个候选更符合'旁边'？"   │   │
│  │        bev_result = VLM(annotated_bev, bev_prompt)           │   │
│  │                                                              │   │
│  │  3.5c. 更新候选排序:                                         │   │
│  │        根据俯视图分析结果调整候选的优先级                    │   │
│  │        filtered_pairs = rerank_by_bev_result(valid_pairs,    │   │
│  │                                               bev_result)    │   │
│  │                                                              │   │
│  │  输出: [(台灯A, 沙发A, 0.6m, bev_score=0.95),                │   │
│  │         (台灯B, 沙发A, 1.2m, bev_score=0.3)]                 │   │
│  │                                                              │   │
│  │  优势:                                                       │   │
│  │  • 一次VLM调用(~200ms)可快速筛选大量候选                     │   │
│  │  • 利用俯视图的全局布局信息                                  │   │
│  │  • 减少后续RGB视角验证的候选数量                             │   │
│  └─────────────────────────────────────────────────────────────┘   │
│         │                                                           │
│  ═══════╪═══════════════════════════════════════════════════════   │
│         ▼                                                           │
│  Step 4: 视角选择 (双向索引)                                        │
│  ┌─────────────────────────────────────────────────────────────┐   │
│  │  需要同时看到 target 和 anchor 的视角:                       │   │
│  │                                                              │   │
│  │  view_ids = visibility_index.get_best_views_for_objects(     │   │
│  │      object_ids=[台灯A.id, 沙发A.id, 台灯B.id],              │   │
│  │      criteria="joint_coverage",                              │   │
│  │      max_views=3                                             │   │
│  │  )                                                           │   │
│  │  → [view_42, view_67, view_89]                               │   │
│  └─────────────────────────────────────────────────────────────┘   │
│         │                                                           │
│  ═══════╪═══════════════════════════════════════════════════════   │
│         ▼                                                           │
│  Step 5: VLM输入构造                                                 │
│  ┌─────────────────────────────────────────────────────────────┐   │
│  │  根据 query_type="spatial_relation" 构造输入:                │   │
│  │                                                              │   │
│  │  images = [rgb_images[42], rgb_images[67], rgb_images[89]]   │   │
│  │  bev = bev_image_with_annotations(候选物体标注)  # 仅当use_bev=true │   │
│  │                                                              │   │
│  │  text_context = """                                          │   │
│  │  候选物体:                                                   │   │
│  │  - #12 台灯A: 沙发左侧50cm，白色金属台灯，高60cm             │   │
│  │  - #15 台灯B: 沙发右侧120cm，黑色落地灯，高150cm             │   │
│  │  - #3 沙发A: 客厅中央，灰色布艺三人沙发                      │   │
│  │  """                                                         │   │
│  │                                                              │   │
│  │  prompt = "请输出JSON: {selected_id: <ID>, confidence: ...}" │   │
│  │                                                              │   │
│  │  注: images上已标注 #12, #15, #3 等ID (视觉提示)             │   │
│  └─────────────────────────────────────────────────────────────┘   │
│         │                                                           │
│  ═══════╪═══════════════════════════════════════════════════════   │
│         ▼                                                           │
│  Step 6: VLM推理                                                     │
│  ┌─────────────────────────────────────────────────────────────┐   │
│  │  VLM输入: images(带ID标注) + bev + text_context + prompt     │   │
│  │                                                              │   │
│  │  VLM输出 (文本): {                                           │   │
│  │    "selected_id": 12,                                        │   │
│  │    "confidence": 0.92,                                       │   │
│  │    "reasoning": "台灯A距离沙发更近(约50cm)，且位于沙发      │   │
│  │                  左扶手旁边，符合'旁边'的描述。"             │   │
│  │  }                                                           │   │
│  └─────────────────────────────────────────────────────────────┘   │
│         │                                                           │
│  ═══════╪═══════════════════════════════════════════════════════   │
│         ▼                                                           │
│  Step 7: 输出解析 (文本 → 3D场景映射)                               │
│  ┌─────────────────────────────────────────────────────────────┐   │
│  │  7a. 解析VLM输出:                                            │   │
│  │      parsed = json.loads(vlm_output)                         │   │
│  │      object_id = parsed["selected_id"]  → 12                 │   │
│  │                                                              │   │
│  │  7b. 查找物体表 (OLT):                                       │   │
│  │      object_node = OLT[12]  → ObjectNode(台灯A)              │   │
│  │                                                              │   │
│  │  7c. 提取3D信息:                                             │   │
│  │      bbox_3d = object_node.bbox_3d                           │   │
│  │      centroid = object_node.centroid  → [1.2, 3.4, 0.6]      │   │
│  │      point_cloud = object_node.point_cloud                   │   │
│  └─────────────────────────────────────────────────────────────┘   │
│         │                                                           │
│  ═══════╪═══════════════════════════════════════════════════════   │
│         ▼                                                           │
│  最终输出: GroundingResult                                          │
│  {                                                                  │
│    "success": true,                                                 │
│    "object_id": 12,                                                 │
│    "object_node": ObjectNode(台灯A),                                │
│    "bbox_3d": BoundingBox3D(...),                                   │
│    "centroid": [1.2, 3.4, 0.6],                                     │
│    "point_cloud": PointCloud(...),                                  │
│    "confidence": 0.92,                                              │
│    "reasoning": "台灯A距离沙发更近..."                              │
│  }                                                                  │
│                                                                     │
└─────────────────────────────────────────────────────────────────────┘
```

### 4.2 查询解析与use_bev推断

```python
class QueryParser:
    """查询解析器 - LLM推断查询类型和是否需要俯视图"""
    
    def parse(self, query: str) -> QueryInfo:
        """
        让LLM解析查询，返回结构化信息
        关键: use_bev由LLM根据查询内容自动推断
        """
        
        prompt = f"""
        分析以下查询，提取结构化信息:
        
        查询: "{query}"
        
        请输出JSON格式:
        {{
            "target": "<目标物体>",
            "anchor": "<参考物体，如无则为null>",
            "relation": "<空间关系，如无则为null>",
            "query_type": "<simple_object|spatial_relation|functional_region|counting|attribute|comparison>",
            "use_bev": <true|false>
        }}
        
        use_bev判断标准:
 - true: 涉及水平空间关系(旁边/之间/周围)、区域识别、计数、需要全局布局理解
 - false: 简单物体定位、垂直关系(上面/下面)、外观属性、物体比较
        
        示例:
        "沙发旁边的台灯" → use_bev: true (水平空间关系)
        "桌子上的杯子" → use_bev: false (垂直关系)
        "红色的椅子" → use_bev: false (外观属性)
        "房间里有多少把椅子" → use_bev: true (计数)
        "适合阅读的角落" → use_bev: true (区域识别)
        """
        
        return self.llm.parse(prompt)


class QueryAdaptiveInputConstructor:
    """查询自适应的VLM输入构造器"""
    
    # 注意: bev字段现在由query_info.use_bev决定，不再硬编码
    STRATEGY_MAP = {
        "simple_object": {
            # 查询: "找到红色的杯子"
            "views": "best_single",           # 最清晰的单张视角
            "descriptions": "target_only",    # 只需目标物体描述
            "max_views": 1,
        },
        "spatial_relation": {
            # 查询: "沙发旁边的台灯"
            "views": "joint_coverage",        # 同时覆盖target和anchor
            "descriptions": "all_candidates", # 所有候选物体描述
            "max_views": 3,
        },
        "functional_region": {
            # 查询: "适合阅读的地方"
            "views": "region_overview",       # 区域全景
            "descriptions": "region_summary", # 区域功能描述
            "max_views": 2,
        },
        "counting": {
            # 查询: "房间里有多少把椅子"
            "views": "multi_angle",           # 多角度避免遮挡漏数
            "descriptions": "enumeration",    # 枚举所有候选
            "max_views": 4,
        },
        "attribute": {
            # 查询: "杯子是什么颜色"
            "views": "close_up",              # 近距离特写
            "descriptions": "detailed",       # 详细属性描述
            "max_views": 2,
        },
        "comparison": {
            # 查询: "哪把椅子更高"
            "views": "same_frame",            # 尽量在同一帧中比较
            "descriptions": "comparative",    # 对比式描述
            "max_views": 2,
        },
    }
    
    def construct(self, query: str, 
                  query_info: QueryInfo,  # 包含use_bev字段
                  candidates: List[ObjectNode],
                  scene_repr: QuerySceneRepresentation) -> VLMInput:
        
        query_type = query_info["query_type"]
        use_bev = query_info["use_bev"]  # 由LLM在解析时推断
        strategy = self.STRATEGY_MAP[query_type]
        
        # 1. 获取相关视角
        if strategy["views"] == "joint_coverage":
            all_obj_ids = [c.obj_id for c in candidates]
            view_ids = scene_repr.visibility_index.get_best_views_for_objects(
                all_obj_ids, criteria="joint_coverage", max_views=strategy["max_views"]
            )
        elif strategy["views"] == "best_single":
            target_obj = candidates[0]
            view_ids = [scene_repr.visibility_index.object_to_views[target_obj.obj_id][0][0]]
        # ... 其他策略
        
        # 2. 准备图像
        images = [scene_repr.rgb_images[v] for v in view_ids]
        
        # 3. 准备俯视图 (关键改动: 由query_info.use_bev决定)
        bev = None
        if use_bev:
            bev = self._annotate_bev(scene_repr.bev_image, candidates, query_info)
        
        # 4. 准备文本描述
        descriptions = self._prepare_descriptions(candidates, strategy["descriptions"])
        
        # 5. 生成prompt
        prompt = self._generate_prompt(query, query_type, candidates, use_bev)
        
        return VLMInput(
            images=images,
            bev=bev,  # 可能为None
            descriptions=descriptions,
            prompt=prompt,
            view_ids=view_ids,
        )
    
    def _annotate_bev(self, bev_image, candidates, query_info):
        """生成带标注的俯视图"""
        annotated = bev_image.copy()
        
        # 1. 如果有anchor，用红色高亮
        if query_info.get("anchor"):
            anchor_obj = self._find_object_by_name(query_info["anchor"], candidates)
            if anchor_obj:
                pos = self._project_to_bev(anchor_obj.centroid)
                cv2.circle(annotated, pos, radius=20, color=(0,0,255), thickness=-1)
                cv2.putText(annotated, f"参考: {anchor_obj.category}", pos, ...)
                
                # 绘制距离参考圈
                cv2.circle(annotated, pos, self._meters_to_pixels(1.0), (128,128,128), 1)
                cv2.circle(annotated, pos, self._meters_to_pixels(2.0), (128,128,128), 1)
        
        # 2. 用蓝色标注所有候选目标
        for i, obj in enumerate(candidates):
            if obj != anchor_obj:
                pos = self._project_to_bev(obj.centroid)
                cv2.circle(annotated, pos, radius=15, color=(255,0,0), thickness=-1)
                cv2.putText(annotated, f"#{i+1}: {obj.category}", pos, ...)
        
        return annotated
```

### 4.3 俯视图分支的触发逻辑

```python
def query_pipeline(self, query: str, scene_repr: QuerySceneRepresentation):
    """完整查询流程，包含可选的俯视图分支"""
    
    # Step 1: 查询解析 (LLM推断use_bev)
    query_info = self.query_parser.parse(query)
    
    # Step 2: 层次化检索
    candidates = self.hierarchical_retrieval(query_info, scene_repr)
    
    # Step 3: 空间约束过滤
    filtered_candidates = self.spatial_filter(candidates, query_info)
    
    # Step 3.5: 俯视图空间筛选 [可选分支]
    if query_info["use_bev"] and len(filtered_candidates) > 2:
        # 候选较多时，用俯视图快速筛选
        filtered_candidates = self.bev_spatial_filter(
            filtered_candidates, query_info, scene_repr
        )
    
    # Step 4: 视角选择
    view_ids = self.select_best_views(filtered_candidates, scene_repr)
    
    # Step 5: VLM输入构造 (use_bev决定是否包含俯视图)
    vlm_input = self.input_constructor.construct(
        query, query_info, filtered_candidates, scene_repr
    )
    
    # Step 6: VLM推理
    result = self.vlm.infer(vlm_input)
    
    # Step 7: 输出解析 - 将VLM文本输出映射回场景
    grounding_result = self.output_parser.parse(result, filtered_candidates)
    
    return grounding_result
```

### 4.4 VLM输出解析与场景映射

VLM的输出是文本，需要解析并映射回3D场景中的具体物体。基于SeeGround等最新工作，采用**物体ID查找表(OLT)**方法。

#### 4.4.1 核心思路

```
┌─────────────────────────────────────────────────────────────────────┐
│                    VLM输出 → 场景映射流程                            │
├─────────────────────────────────────────────────────────────────────┤
│                                                                     │
│  预构建: Object Lookup Table (OLT)                                  │
│  ┌─────────────────────────────────────────────────────────────┐   │
│  │  OLT = {                                                     │   │
│  │    #1: {bbox_3d: [...], centroid: [...], category: "沙发"},  │   │
│  │    #2: {bbox_3d: [...], centroid: [...], category: "台灯"},  │   │
│  │    #3: {bbox_3d: [...], centroid: [...], category: "茶几"},  │   │
│  │    ...                                                       │   │
│  │  }                                                           │   │
│  └─────────────────────────────────────────────────────────────┘   │
│                              ↓                                      │
│  VLM输入: 图像上标注物体ID + 结构化Prompt                           │
│  ┌─────────────────────────────────────────────────────────────┐   │
│  │  [图像: #1沙发, #2台灯A, #3台灯B 标注在对应位置]             │   │
│  │  Prompt: "请输出JSON格式: {selected_id: <ID>, ...}"         │   │
│  └─────────────────────────────────────────────────────────────┘   │
│                              ↓                                      │
│  VLM输出: {"selected_id": 2, "confidence": 0.9, ...}               │
│                              ↓                                      │
│  解析: OLT[2] → 3D bbox, centroid, point_cloud                     │
│                                                                     │
└─────────────────────────────────────────────────────────────────────┘
```

#### 4.4.2 输出解析器实现

```python
class VLMOutputParser:
    """VLM输出解析器 - 将文本输出映射回3D场景"""
    
    def __init__(self, scene_repr: QuerySceneRepresentation):
        # 构建物体ID查找表 (OLT)
        self.object_lookup = {
            obj.obj_id: obj for obj in scene_repr.objects
        }
    
    def parse(self, vlm_output: str, candidates: List[ObjectNode]) -> GroundingResult:
        """解析VLM输出，返回3D定位结果"""
        
        # 方法1: 结构化JSON解析 (首选)
        if self._is_json(vlm_output):
            parsed = json.loads(self._extract_json(vlm_output))
            obj_id = parsed["selected_id"]
            confidence = parsed.get("confidence", 1.0)
            reasoning = parsed.get("reasoning", "")
            return self._lookup_object(obj_id, confidence, reasoning)
        
        # 方法2: 正则提取ID (fallback)
        obj_id = self._extract_id_from_text(vlm_output, candidates)
        if obj_id is not None:
            return self._lookup_object(obj_id, confidence=0.8)
        
        # 方法3: 描述文本匹配 (最后手段)
        matched_obj = self._match_by_description(vlm_output, candidates)
        if matched_obj:
            return self._create_result(matched_obj, confidence=0.6)
        
        return GroundingResult(success=False, reason="无法解析VLM输出")
    
    def _extract_id_from_text(self, text: str, candidates: List[ObjectNode]) -> Optional[int]:
        """从文本中提取物体ID"""
        candidate_ids = {c.obj_id for c in candidates}
        
        # 模式1: "#数字" 格式 → "#2台灯A" → 2
        for match in re.finditer(r'#(\d+)', text):
            obj_id = int(match.group(1))
            if obj_id in candidate_ids:
                return obj_id
        
        # 模式2: "ID: 数字" 或 "物体ID: 数字"
        match = re.search(r'(?:物体|ID|id)[:\s]*(\d+)', text)
        if match:
            obj_id = int(match.group(1))
            if obj_id in candidate_ids:
                return obj_id
        
        # 模式3: "候选X" (X是1-indexed序号)
        match = re.search(r'候选[:\s]*(\d+)', text)
        if match:
            idx = int(match.group(1)) - 1
            if 0 <= idx < len(candidates):
                return candidates[idx].obj_id
        
        return None
    
    def _lookup_object(self, obj_id: int, confidence: float = 1.0, 
                       reasoning: str = "") -> GroundingResult:
        """根据ID查找物体，返回完整3D信息"""
        
        obj = self.object_lookup.get(obj_id)
        if obj is None:
            return GroundingResult(success=False, reason=f"未找到ID={obj_id}的物体")
        
        return GroundingResult(
            success=True,
            object_id=obj_id,
            object_node=obj,
            bbox_3d=obj.bbox_3d,
            centroid=obj.centroid,
            point_cloud=obj.point_cloud,
            confidence=confidence,
            reasoning=reasoning,
        )


@dataclass
class GroundingResult:
    """3D定位结果"""
    success: bool
    object_id: Optional[int] = None
    object_node: Optional[ObjectNode] = None
    bbox_3d: Optional[BoundingBox3D] = None
    centroid: Optional[np.ndarray] = None      # [x, y, z]
    point_cloud: Optional[PointCloud] = None
    confidence: float = 0.0
    reasoning: str = ""
    reason: str = ""  # 失败原因
```

#### 4.4.3 结构化Prompt设计

确保VLM输出可解析的关键是设计好prompt：

```python
def generate_grounding_prompt(query: str, candidates: List[ObjectNode]) -> str:
    """生成要求结构化输出的prompt"""
    
    candidate_list = "\n".join([
        f"  #{obj.obj_id}: {obj.category} - {obj.descriptions.summary}"
        for obj in candidates
    ])
    
    return f"""
任务: {query}

候选物体 (只能从以下ID中选择):
{candidate_list}

请严格按照以下JSON格式输出，不要添加任何其他内容:
{{
    "selected_id": <必须是上述#后面的数字之一>,
    "confidence": <0到1之间的置信度>,
    "reasoning": "<简短的推理过程>"
}}
"""
```

#### 4.4.4 图像标注：视觉提示

在输入图像上标注物体ID，帮助VLM建立ID与视觉内容的对应：

```python
def annotate_image_with_ids(image: np.ndarray, 
                            candidates: List[ObjectNode],
                            camera_pose: CameraPose,
                            K: np.ndarray) -> np.ndarray:
    """在图像上标注物体ID"""
    
    annotated = image.copy()
    
    for obj in candidates:
        # 将3D边界框投影到2D
        bbox_2d = project_3d_to_2d(obj.bbox_3d, camera_pose, K)
        
        if bbox_2d is not None and is_visible(bbox_2d, image.shape):
            # 绘制边界框
            cv2.rectangle(annotated, 
                         (int(bbox_2d[0]), int(bbox_2d[1])),
                         (int(bbox_2d[2]), int(bbox_2d[3])),
                         color=(0, 255, 0), thickness=2)
            
            # 标注ID和类别
            label = f"#{obj.obj_id}: {obj.category}"
            cv2.putText(annotated, label,
                       (int(bbox_2d[0]), int(bbox_2d[1]) - 10),
                       cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 255, 0), 2)
    
    return annotated
```

#### 4.4.5 多级解析策略

```
┌─────────────────────────────────────────────────────────────────────┐
│                    多级解析策略                                      │
├─────────────────────────────────────────────────────────────────────┤
│                                                                     │
│  VLM输出文本                                                        │
│       │                                                             │
│       ▼                                                             │
│  Level 1: JSON解析 (置信度最高)                                     │
│  ┌─────────────────────────────────────────────────────────────┐   │
│  │  尝试解析为JSON: {"selected_id": 2, "confidence": 0.9}      │   │
│  │  成功 → 返回结果 (confidence保留VLM给出的值)                │   │
│  │  失败 → 进入Level 2                                         │   │
│  └─────────────────────────────────────────────────────────────┘   │
│       │                                                             │
│       ▼                                                             │
│  Level 2: 正则提取ID (置信度中等)                                   │
│  ┌─────────────────────────────────────────────────────────────┐   │
│  │  匹配模式: #数字, ID:数字, 候选X                            │   │
│  │  成功 → 返回结果 (confidence=0.8)                           │   │
│  │  失败 → 进入Level 3                                         │   │
│  └─────────────────────────────────────────────────────────────┘   │
│       │                                                             │
│       ▼                                                             │
│  Level 3: 描述匹配 (置信度较低)                                     │
│  ┌─────────────────────────────────────────────────────────────┐   │
│  │  将VLM输出与候选物体的描述计算相似度                        │   │
│  │  成功 → 返回结果 (confidence=0.6)                           │   │
│  │  失败 → 返回解析失败                                        │   │
│  └─────────────────────────────────────────────────────────────┘   │
│                                                                     │
└─────────────────────────────────────────────────────────────────────┘
```

---

## 五、离线构建流程

### 5.1 完整构建流程

```
┌─────────────────────────────────────────────────────────────────────┐
│                    QueryScene 离线构建流程                           │
├─────────────────────────────────────────────────────────────────────┤
│                                                                     │
│  输入: RGB-D视频序列                                                 │
│         │                                                           │
│  ═══════╪═══════════════════════════════════════════════════════   │
│         ▼                                                           │
│  Stage 1: 3D重建与位姿估计                                          │
│  ┌─────────────────────────────────────────────────────────────┐   │
│  │  • SLAM/SfM → 相机位姿序列                                   │   │
│  │  • TSDF融合 → 完整3D点云                                     │   │
│  │  • 地面检测 → 地面平面方程                                   │   │
│  └─────────────────────────────────────────────────────────────┘   │
│         │                                                           │
│  ═══════╪═══════════════════════════════════════════════════════   │
│         ▼                                                           │
│  Stage 2: 物体检测与分割                                             │
│  ┌─────────────────────────────────────────────────────────────┐   │
│  │  • GroundingDINO → 2D检测框 + 类别标签                       │   │
│  │  • SAM → 2D分割掩码                                          │   │
│  │  • 深度反投影 → 物体3D点云                                   │   │
│  │  • 多视角融合 → 完整物体点云                                 │   │
│  └─────────────────────────────────────────────────────────────┘   │
│         │                                                           │
│  ═══════╪═══════════════════════════════════════════════════════   │
│         ▼                                                           │
│  Stage 3: 区域划分                                                   │
│  ┌─────────────────────────────────────────────────────────────┐   │
│  │  方案A (有明显墙体):                                         │   │
│  │    • 墙体检测 (RANSAC平面拟合)                               │   │
│  │    • 墙线提取 → 2D分割线                                     │   │
│  │    • 空间分割 → 区域多边形                                   │   │
│  │                                                              │   │
│  │  方案B (开放式布局):                                         │   │
│  │    • 物体功能聚类 (沙发+茶几+电视 → 客厅区)                  │   │
│  │    • 空间邻近聚类                                            │   │
│  │    • 生成凸包作为区域边界                                    │   │
│  └─────────────────────────────────────────────────────────────┘   │
│         │                                                           │
│  ═══════╪═══════════════════════════════════════════════════════   │
│         ▼                                                           │
│  Stage 4: 特征提取                                                   │
│  ┌─────────────────────────────────────────────────────────────┐   │
│  │  • 逐帧CLIP特征 → 反投影到3D → 逐点CLIP特征                  │   │
│  │  • 物体特征聚合: 点特征平均池化 → 物体CLIP特征               │   │
│  │  • 区域特征聚合: 物体特征加权平均 → 区域CLIP特征             │   │
│  └─────────────────────────────────────────────────────────────┘   │
│         │                                                           │
│  ═══════╪═══════════════════════════════════════════════════════   │
│         ▼                                                           │
│  Stage 5: 描述生成 (VLM)                                            │
│  ┌─────────────────────────────────────────────────────────────┐   │
│  │  对每个物体:                                                 │   │
│  │    • 选择最佳视角的裁剪图                                    │   │
│  │    • VLM生成多层次描述 (外观/功能/空间/上下文)               │   │
│  │    • 编码描述文本 → text_embedding                           │   │
│  │                                                              │   │
│  │  对每个区域:                                                 │   │
│  │    • 生成区域功能描述                                        │   │
│  │    • 确定区域类型标签                                        │   │
│  └─────────────────────────────────────────────────────────────┘   │
│         │                                                           │
│  ═══════╪═══════════════════════════════════════════════════════   │
│         ▼                                                           │
│  Stage 6: 索引构建                                                   │
│  ┌─────────────────────────────────────────────────────────────┐   │
│  │  • 层次化CLIP索引: 区域/物体/点三级FAISS索引                 │   │
│  │  • 视角-物体索引: 计算所有(物体, 视角)对的可见性             │   │
│  │  • 空间索引: 物体质心的KD-Tree                               │   │
│  └─────────────────────────────────────────────────────────────┘   │
│         │                                                           │
│  ═══════╪═══════════════════════════════════════════════════════   │
│         ▼                                                           │
│  Stage 7: 俯视图生成                                                 │
│  ┌─────────────────────────────────────────────────────────────┐   │
│  │  • 点云俯视投影 → 基础BEV图                                  │   │
│  │  • 叠加物体边界框和标签                                      │   │
│  │  • 叠加区域边界和类型                                        │   │
│  └─────────────────────────────────────────────────────────────┘   │
│         │                                                           │
│  ═══════╪═══════════════════════════════════════════════════════   │
│         ▼                                                           │
│  输出: QuerySceneRepresentation                                     │
│                                                                     │
└─────────────────────────────────────────────────────────────────────┘
```

---

## 六、批判性分析

### 6.1 技术风险与不足

#### 风险1: 可见性计算的准确性 [高风险]

**问题描述**:

```
基于深度图的可见性检查假设深度准确，但实际中:
• RGB-D相机深度噪声大 (Kinect典型误差 ±1-3cm)
• 反光/透明物体深度失效 (玻璃杯、镜子)
• 深度边缘不连续导致投影错误
• 遮挡判断需要完整深度，但输入深度可能有空洞
```

**影响**: 视角选择质量下降，VLM可能看到物体被错误遮挡或视角实际不佳

**改进方案**:

```python
# 方案A: 多视角一致性验证
def robust_visibility_estimation(obj, candidate_views, depth_maps, rgb_images):
    """通过多视角RGB一致性验证可见性"""
    consistent_views = []
    for view_id in candidate_views:
        # 1. 深度投影估计
        depth_visibility = compute_depth_visibility(obj, view_id, depth_maps[view_id])
        
        # 2. RGB特征一致性验证
        # 将物体投影到该视角，检查投影区域的CLIP特征与物体特征的一致性
        projected_region = project_object_to_view(obj, view_id)
        region_clip = extract_clip_feature(rgb_images[view_id], projected_region)
        feature_consistency = cosine_similarity(region_clip, obj.clip_feature)
        
        # 3. 融合判断
        if depth_visibility > 0.3 and feature_consistency > 0.7:
            consistent_views.append(view_id)
    
    return consistent_views

# 方案B: 使用神经渲染的可见性 (如果有3DGS表示)
# 3DGS的splatting权重直接反映可见性，无需显式深度计算
```

#### 风险2: 区域划分的鲁棒性 [中风险]

**问题描述**:

```
• 开放式布局（如loft公寓）没有明显墙体分隔
• 墙体检测依赖平面拟合，曲面墙/玻璃墙难以检测
• 功能聚类需要预定义物体-功能映射，难以泛化
• 同一物理空间可能有多种功能划分方式
```

**影响**: 区域级索引失效，检索效率下降

**改进方案**:

```python
# 方案: 自适应区域划分 + 软边界
class AdaptiveRegionPartitioner:
    def partition(self, point_cloud, objects, walls=None):
        # 1. 尝试基于墙体的硬边界划分
        if walls and len(walls) > 2:
            regions = self.partition_by_walls(walls)
            if self.validate_regions(regions, objects):
                return regions
        
        # 2. 回退到基于物体聚类的软边界划分
        # 使用物体功能特征 + 空间位置进行谱聚类
        object_features = np.concatenate([
            obj.clip_feature,  # 语义特征
            obj.centroid,      # 空间位置
        ], axis=-1)
        
        # 自动确定聚类数量
        n_clusters = self.estimate_num_regions(object_features)
        
        labels = SpectralClustering(n_clusters=n_clusters).fit_predict(object_features)
        
        # 3. 为每个聚类生成凸包作为软边界
        regions = []
        for cluster_id in range(n_clusters):
            cluster_objects = [obj for obj, l in zip(objects, labels) if l == cluster_id]
            boundary = self.compute_convex_hull([obj.centroid[:2] for obj in cluster_objects])
            regions.append(RegionNode(boundary_2d=boundary, object_ids=[o.obj_id for o in cluster_objects]))
        
        return regions
```

#### 风险3: VLM推理的不确定性 [高风险]

**问题描述**:

```
• VLM对空间关系理解可能不准确 ("旁边" vs "附近" 的界定模糊)
• VLM输出不稳定，相同输入可能产生不同结果
• 长文本描述可能导致VLM忽略关键信息
• VLM可能产生幻觉，声称看到了实际不存在的物体
```

**影响**: 最终定位结果不可靠

**改进方案**:

```python
# 方案A: 多次采样 + 一致性投票
def robust_vlm_inference(vlm_input, n_samples=3, temperature=0.3):
    results = []
    for _ in range(n_samples):
        result = vlm.infer(vlm_input, temperature=temperature)
        results.append(result["selected_id"])
    
    # 多数投票
    from collections import Counter
    vote_counts = Counter(results)
    best_id, count = vote_counts.most_common(1)[0]
    
    confidence = count / n_samples
    return best_id, confidence

# 方案B: 几何验证 + VLM置信度融合
def hybrid_inference(vlm_result, geometric_scores, candidates):
    """融合VLM判断和几何计算"""
    
    final_scores = {}
    for candidate in candidates:
        # VLM给出的置信度
        vlm_score = vlm_result.get(candidate.obj_id, 0.0)
        
        # 几何计算的得分 (例如距离满足"旁边"约束的程度)
        geo_score = geometric_scores.get(candidate.obj_id, 0.0)
        
        # 加权融合
        final_scores[candidate.obj_id] = 0.6 * vlm_score + 0.4 * geo_score
    
    best_id = max(final_scores, key=final_scores.get)
    return best_id, final_scores[best_id]

# 方案C: 结构化输出约束
def structured_vlm_prompt(candidates, query):
    """强制VLM输出结构化JSON，减少幻觉"""
    return f"""
任务: {query}

候选物体 (只能从以下ID中选择):
{json.dumps([{"id": c.obj_id, "category": c.category, "description": c.descriptions.summary} for c in candidates], ensure_ascii=False, indent=2)}

请严格按以下JSON格式输出，不要添加任何其他内容:
{{"selected_id": <必须是上述候选ID之一>, "confidence": <0-1的浮点数>, "reason": "<简短理由>"}}
"""
```

#### 风险4: 预构建描述的质量问题 [中风险]

**问题描述**:

```
• VLM生成的描述可能包含幻觉 (描述了不存在的属性)
• 不同物体的描述风格不一致
• 空间描述可能与实际几何不符 ("左侧" 的参考系不明确)
• 描述冗余可能增加VLM推理负担
```

**影响**: 错误的描述误导VLM推理

**改进方案**:

```python
# 方案: 多视角验证 + 几何校验
def generate_verified_description(obj, scene_repr, vlm):
    # 1. 从多个视角生成描述
    descriptions_per_view = []
    for view_id in obj.best_view_ids[:3]:
        crop = obj.crop_images[view_id]
        desc = vlm.describe_object(crop)
        descriptions_per_view.append(desc)
    
    # 2. 提取一致的属性 (多数投票)
    colors = [extract_color(d) for d in descriptions_per_view]
    consistent_color = majority_vote(colors)
    
    materials = [extract_material(d) for d in descriptions_per_view]
    consistent_material = majority_vote(materials)
    
    # 3. 几何信息校验
    # 空间描述使用精确计算，不依赖VLM
    spatial_desc = generate_spatial_description_from_geometry(obj, scene_repr)
    
    # 4. 组合最终描述
    return ObjectDescriptions(
        appearance=f"{consistent_color} {consistent_material} {obj.category}",
        function=vlm.describe_function(obj.category),  # 功能可以用类别推断
        spatial=spatial_desc,  # 几何计算，不用VLM
        context=generate_context_from_neighbors(obj, scene_repr),
        summary=f"{spatial_desc}的{consistent_color}{obj.category}"
    )
```

#### 风险5: 计算和存储开销 [中风险]

**问题描述**:

```
存储估算 (典型室内场景):
• 点云: 100万点 × 512维 × 4字节 = 2GB
• 物体: 200个 × (特征512维 + 描述 + 索引) ≈ 50MB
• 区域: 10个 × 特征 ≈ 5MB
• 视角索引: 1000帧 × 200物体 × 8字节 = 1.6MB
• RGB图像: 1000帧 × 1MB = 1GB
• 总计: ~3-4GB/场景

查询延迟估算:
• CLIP文本编码: ~10ms
• 层次化检索: ~5ms
• 视角选择: ~2ms
• VLM推理: 500ms-2s (取决于模型和图像数量)
• 总计: ~0.5-2s/查询
```

**影响**: 大规模场景或实时应用受限

**改进方案**:

```python
# 方案A: 特征压缩
class CompressedFeatureStore:
    def __init__(self, compression_ratio=8):
        # 使用PQ (Product Quantization) 压缩特征
        # 512维 → 64个子空间 × 8bit = 64字节 (vs 原始2048字节)
        self.pq = faiss.IndexPQ(512, 64, 8)
    
    def add(self, features):
        self.pq.train(features)
        self.pq.add(features)
    
    def search(self, query, k=10):
        return self.pq.search(query, k)

# 方案B: 惰性加载
class LazySceneRepresentation:
    def __init__(self, scene_path):
        self.scene_path = scene_path
        # 只加载索引，不加载原始数据
        self.clip_index = load_index(f"{scene_path}/clip_index.faiss")
        self.visibility_index = load_pickle(f"{scene_path}/visibility_index.pkl")
        
        # 图像惰性加载
        self._rgb_images = None
    
    @property
    def rgb_images(self):
        if self._rgb_images is None:
            self._rgb_images = LazyImageLoader(f"{self.scene_path}/images/")
        return self._rgb_images

# 方案C: 图像缓存 + 预取
class SmartImageLoader:
    def __init__(self, image_dir, cache_size=50):
        self.image_dir = image_dir
        self.cache = LRUCache(cache_size)
    
    def prefetch(self, view_ids):
        """预取即将使用的图像"""
        for view_id in view_ids:
            if view_id not in self.cache:
                self.cache[view_id] = self._load_image(view_id)
```

#### 风险6: 空间关系的模糊性 [中风险]

**问题描述**:

```
• "旁边"、"附近"、"靠近" 的界定因场景而异
• 3D空间关系比2D更复杂 (上下/前后/左右)
• "桌子旁边" 可能指桌面旁还是桌腿旁
• 参考系不明确 (观察者视角 vs 物体固有方向)
```

**影响**: 空间约束过滤可能过严或过松

**改进方案**:

```python
# 方案: 模糊空间推理 + 上下文自适应阈值
class FuzzySpatialReasoning:
    # 基于场景统计的自适应阈值
    RELATION_THRESHOLDS = {
        "旁边": {"base": 1.0, "scale_factor": 0.3},  # 基础1m，按场景大小缩放
        "附近": {"base": 2.0, "scale_factor": 0.3},
        "上面": {"vertical_threshold": 0.1},  # 垂直距离阈值
        "下面": {"vertical_threshold": 0.1},
    }
    
    def check_relation(self, target, anchor, relation, scene_scale):
        if relation in ["旁边", "附近"]:
            threshold = (self.RELATION_THRESHOLDS[relation]["base"] + 
                        scene_scale * self.RELATION_THRESHOLDS[relation]["scale_factor"])
            
            horizontal_dist = np.linalg.norm(target.centroid[:2] - anchor.centroid[:2])
            vertical_dist = abs(target.centroid[2] - anchor.centroid[2])
            
            # 模糊隶属度 (0-1)
            horizontal_membership = max(0, 1 - horizontal_dist / threshold)
            vertical_penalty = min(1, vertical_dist / 0.5)  # 垂直差异大则惩罚
            
            return horizontal_membership * (1 - 0.3 * vertical_penalty)
        
        elif relation == "上面":
            # 检查target在anchor上方
            if target.centroid[2] > anchor.centroid[2] + self.RELATION_THRESHOLDS["上面"]["vertical_threshold"]:
                # 还要检查水平投影有重叠
                horizontal_overlap = compute_bbox_overlap_2d(target.bbox_3d, anchor.bbox_3d)
                return horizontal_overlap
            return 0.0
```

### 6.2 与现有方法的详细对比

| 维度 | QueryScene | View-on-Graph | 3DGraphLLM | OpenScene | VLM-Grounder |

|------|-----------|---------------|------------|-----------|--------------|

| **表示形式** | 多模态索引 | 多层图 | Token化图 | 点特征 | 2D检测 |

| **检索效率** | O(log N) | O(N) 遍历 | O(N) 编码 | O(N) | O(帧数) |

| **VLM接口** | 原生图像 | 原生图像 | Token序列 | 无 | 原生图像 |

| **空间推理** | 几何+VLM | VLM | LLM | 无 | 仅2D |

| **可解释性** | 高 | 高 | 低 | 低 | 高 |

| **预构建成本** | 高 | 中 | 中 | 低 | 低 |

| **大规模扩展** | 好 | 差 | 中 | 中 | 中 |

| **动态更新** | 需重建索引 | 图更新 | 需重编码 | 点更新 | 无需 |

### 6.3 核心创新点与贡献总结

1. **视角-物体双向索引** (核心贡献)

                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                - 首次将视角检索作为场景表示的一等公民
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                - 支持O(1)复杂度的最佳视角选择
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                - 解决了3D表示到VLM的模态转换问题

2. **查询自适应VLM输入构造**

                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                - 根据查询类型动态选择最优输入策略
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                - 比固定策略更高效、更准确

3. **层次化检索架构**

                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                - 区域→物体→点的三级检索
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                - 大规模场景下保持亚线性复杂度

4. **多层次语言描述**

                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                - 外观/功能/空间/上下文四维描述
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                - 为VLM提供丰富的推理上下文

### 6.4 建议的研究路线

```
Phase 1: 核心验证 [4-6周]
├── 实现基础数据结构和索引
├── 在ScanNet小规模场景上测试
├── 验证视角索引的有效性
└── 对比baseline (OpenScene, VLM-Grounder)

Phase 2: 完整系统 [6-8周]
├── 实现查询解析和自适应输入构造
├── 集成VLM推理模块
├── 在ScanRefer上完整评估
└── 消融实验分析各模块贡献

Phase 3: 优化与扩展 [4-6周]
├── 大规模场景测试 (Matterport3D)
├── 计算效率优化
├── 鲁棒性提升 (遮挡、噪声)
└── 论文撰写
```